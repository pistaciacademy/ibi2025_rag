{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c5ea3f4a75c"
   },
   "source": [
    "# Assistant de recherche avec Gemini, LangChain, and Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "479790a71f3c"
   },
   "source": [
    "## Objectif\n",
    "\n",
    "L'objectif principal de ce workshop est de concevoir un assistant de recherche dédié au traitement de papiers scientifiques. Cet outil permettra d'importer et d'indexer ces documents dans le but de faciliter la réponse à des questions de recherche ultérieures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qRjVe1tZhsx"
   },
   "source": [
    "## Installation des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olK4Ejjzuj76"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q langchain-community\n",
    "!pip install -q langchain-google-genai\n",
    "!pip install -q chromadb\n",
    "!pip install -q pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiGHSFmZaniK"
   },
   "source": [
    "### Génération de Gemeni API Key\n",
    "\n",
    "Pour utiliser les modèles Gemini de Google via une API, il est nécessaire d'obtenir une clé API. Cette clé peut être générée facilement et gratuitement via l'interface de Google AI Studio  [Google AI Studio](https://makersuite.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745930129760,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "xId4sR52utS0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = \"XXXXXXXXXXXX\" # Ici remplacez par votre API Key !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPhs4mDkjdgY"
   },
   "source": [
    "## Import des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1745930131807,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "TcvGPVdXu05F"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os\n",
    "import getpass\n",
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.prompt_template import format_document\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4461Jihk_rWq"
   },
   "source": [
    "## Retriever\n",
    "\n",
    "Cette étape, nommée \"Retriever\", consiste à préparer des pdf pour un modèle linguistique (LLM).\n",
    "\n",
    "1. Lire les fichiers pdf avec LangChain.\n",
    "2. Créer des embeddings (représentations numériques vectorielles) de ces données en utilisant le modèle de Gemini. Les textes similaires auront des vecteurs similaires.\n",
    "3. Stocker ces embeddings dans Chroma, une base de données vectorielle, pour les retrouver facilement.\n",
    "4. Créer un \"Retriever\" à partir de Chroma pour récupérer les informations pertinentes (embeddings) à passer au LLM en fonction des questions de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WomGvIAVjZeI"
   },
   "source": [
    "### Chargement des fichiers pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1745929892545,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "DeNX9QFM0V-C"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def get_file_text(file_path):\n",
    "  loader = PyPDFLoader(file_path)\n",
    "  pages = []\n",
    "  for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "  return format_docs(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6718,
     "status": "ok",
     "timestamp": 1745930035097,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "he-SRefZlbOS"
   },
   "outputs": [],
   "source": [
    "final_text = get_file_text(r\"ici il faut remplacer par le path de votre fichier .pdf\")\n",
    "#print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDsdAg4Fjo5o"
   },
   "source": [
    "### Initialisation du modèle d'embedding (modèle de Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1745930140026,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "8NXNTrjp0jdh"
   },
   "outputs": [],
   "source": [
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9Vzw30wpebs"
   },
   "source": [
    "### Embedding et sauvegarde des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1745930144794,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "n1VwhUQMvpcN"
   },
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    return pages\n",
    "\n",
    "def generate_vectorstore(file_path):\n",
    "    pages = load_file(file_path)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    chunked_documents = text_splitter.split_documents(pages)\n",
    "\n",
    "    print(f\"Chargé {len(pages)} pages et créé {len(chunked_documents)} chunks.\")\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "                        documents=chunked_documents,\n",
    "                        embedding=gemini_embeddings,\n",
    "                        persist_directory=\"./chroma_db\"\n",
    "                    )\n",
    "\n",
    "    print(f\"Base de données vectorielle créée/mise à jour dans ./chroma_db avec {len(chunked_documents)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33385,
     "status": "ok",
     "timestamp": 1745930181944,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "HVuIm4jtmWzL",
    "outputId": "4d757e29-007b-4510-920d-6c6d9d0c72cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargé 599 pages et créé 992 chunks.\n",
      "Base de données vectorielle créée/mise à jour dans ./chroma_db avec 992 chunks.\n"
     ]
    }
   ],
   "source": [
    "generate_vectorstore(r\"ici il faut remplacer par le path de votre fichier .pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFKyb3JXOeaQ"
   },
   "source": [
    "### Creation d'un \"retriever\" avec Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1745930203546,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "s3t4kmzIOZQq"
   },
   "outputs": [],
   "source": [
    "def get_retriver(gemini_embeddings):\n",
    "  vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",\n",
    "                        embedding_function=gemini_embeddings\n",
    "                   )\n",
    "  return vectorstore_disk.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1745930208382,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "PW3H9732m0iD",
    "outputId": "286c55b8-895c-40ec-c13f-4ad14c9125b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-4f3f5f1247b2>:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore_disk = Chroma(\n"
     ]
    }
   ],
   "source": [
    "retriever = get_retriver(gemini_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZwcZyRxSO0q"
   },
   "source": [
    "## Génération de réponses\n",
    "\n",
    "L'étape de génération de réponses consiste à demander une réponse au LLM lorsque l'utilisateur pose une question. Le \"retriever\", créé précédemment à partir de la base de données vectorielle Chroma, fournira au LLM les informations pertinentes (les embeddings du texte) pour donner du contexte à la question de l'utilisateur.\n",
    "\n",
    "Les étapes sont :\n",
    "\n",
    "1. Combiner un prompt pour le retriever, un prompt pour la réponse, et le modèle LLM.\n",
    "2. Exécuter cette combinaison avec la question de l'utilisateur pour obtenir une réponse du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtUi5FBIJMDy"
   },
   "source": [
    "### Initialisation du LLM Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1745930213647,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "CaA1vRCh7s36"
   },
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jC4QDhiPpDJa"
   },
   "source": [
    "### Création du template de prompt\n",
    "\n",
    "Dans le modèle de prompt, la variable \"question\" sera remplacée par la question de l'utilisateur, et la variable \"context\" par les paragraphes récupérés depuis Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1745930219378,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "90Czqh074dEC"
   },
   "outputs": [],
   "source": [
    "def get_prompt():\n",
    "  llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "  Use the following context to answer the question.\n",
    "  If you don't know the answer, just say that you don't know.\n",
    "  Use five sentences maximum and keep the answer concise.\\n\n",
    "  Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "  return PromptTemplate.from_template(llm_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1745930221301,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "rxRkHIJQnTnZ"
   },
   "outputs": [],
   "source": [
    "llm_prompt = get_prompt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXDh2jsdp4sr"
   },
   "source": [
    "### Creation de la chaine de réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745930223543,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "gj5sWzpwp7vc"
   },
   "outputs": [],
   "source": [
    "def get_llm_response(question):\n",
    "  retriever = get_retriver(gemini_embeddings)\n",
    "  llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7, top_p=0.85)\n",
    "  llm_prompt = get_prompt()\n",
    "  rag_chain = (\n",
    "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "      | llm_prompt\n",
    "      | llm\n",
    "      | StrOutputParser()\n",
    "  )\n",
    "\n",
    "  return rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPPqsGCLIrs1"
   },
   "source": [
    "### Test de la solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 7816,
     "status": "ok",
     "timestamp": 1745930667923,
     "user": {
      "displayName": "Pistacia",
      "userId": "12063432311400108299"
     },
     "user_tz": -120
    },
    "id": "4vIaopCsIq0B",
    "outputId": "0db700ec-5beb-4d57-aa52-eb0a8ce91abb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Generative models generate sequences of tokens from sequences of tokens. These models are trained on a wide variety of tasks and usually do not perform your use case out of the box. To guide the model, you can use instructions or prompts. Generative models use autoregressive, token-by-token generation. They can also leverage a tree-based structure to generate intermediate thoughts.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_response(\"Give me a detailed definition of generative ai\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/google/generative-ai-docs/blob/main/examples/gemini/python/langchain/Gemini_LangChain_QA_Chroma_WebLoad.ipynb",
     "timestamp": 1745561314406
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
