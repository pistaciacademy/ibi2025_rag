{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41BK2KLDDUdu"
      },
      "source": [
        "#Evaluation des résultats avec RAGAS\n",
        "\n",
        "L'objectif de ce notebook est d'évaluer les résultats du système RAG créé précédemment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAmzqU3hECCt"
      },
      "source": [
        "##Installation des packages nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3CmOqTh7zoO"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain-community\n",
        "!pip install --quiet langchain-google-genai\n",
        "!pip install --quiet chromadb\n",
        "!pip install -q pypdf\n",
        "!pip install -q ragas\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H8jIC5tEY_-"
      },
      "source": [
        "##Import des packages nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9e5gMyXEhBy",
        "outputId": "8d9e72f1-5a89-4030-9785-48746ab551c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KU1qKgFEwu"
      },
      "source": [
        "##Définition du modèle LLM et du modèle embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZDnvJt5xFFtC"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_API_KEY'] = \"XXXXXXXXXXXX\" # Ici remplacez par votre API Key !!!\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\", temperature=0.7, top_p=0.85)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr5lvkPrF7CO"
      },
      "source": [
        "## Définition du système RAG créé précédemment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o_XfcKp179au"
      },
      "outputs": [],
      "source": [
        "def get_retriver(gemini_embeddings):\n",
        "\n",
        "    vectorstore_disk = Chroma(\n",
        "                         persist_directory=\"./chroma_db\",\n",
        "                         embedding_function=gemini_embeddings\n",
        "                       )\n",
        "    return vectorstore_disk.as_retriever(search_kwargs={\"k\": 3}) # k=3 signifie récupérer les 5 chunks les plus pertinents\n",
        "\n",
        "def get_prompt():\n",
        "\n",
        "    llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use five sentences maximum and keep the answer concise.\\n\n",
        "    Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
        "\n",
        "    return PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def get_llm_response(question):\n",
        "    retriever = get_retriver(gemini_embeddings)\n",
        "    llm_prompt = get_prompt()\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | llm_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain.invoke(question)\n",
        "\n",
        "def load_file(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "    return pages\n",
        "\n",
        "def generate_vectorstore(file_path):\n",
        "\n",
        "    pages = load_file(file_path)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    chunked_documents = text_splitter.split_documents(pages)\n",
        "\n",
        "    print(f\"Chargé {len(pages)} pages et créé {len(chunked_documents)} chunks.\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "                        documents=chunked_documents,\n",
        "                        embedding=gemini_embeddings,\n",
        "                        persist_directory=\"./chroma_db\"\n",
        "                    )\n",
        "\n",
        "    print(f\"Base de données vectorielle créée/mise à jour dans ./chroma_db avec {len(chunked_documents)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoSrk66QGOZI"
      },
      "source": [
        "##Chargement des documents sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvdhITsp9JeR",
        "outputId": "3e86c1d6-d479-40cc-82fc-ed8eb3a28b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargé 25 pages et créé 26 chunks.\n",
            "Base de données vectorielle créée/mise à jour dans ./chroma_db avec 26 chunks.\n"
          ]
        }
      ],
      "source": [
        "generate_vectorstore(r\"ici il faut remplacer par le path de votre fichier .pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_e7WkARJXCD"
      },
      "source": [
        "##Création de dataset de test\n",
        "\n",
        "Pour évaluer la performance du système RAG, nous devons préparer un dataset de test. Ce dataset permettra au package ragas de calculer différentes métriques pour juger de la qualité de la récupération d'information et de la génération de réponse.\n",
        "\n",
        "Ce dataset est structuré autour de plusieurs éléments :\n",
        "\n",
        "- questions : La liste des questions posées au système RAG.\n",
        "\n",
        "- ground_truths : Les réponses \"idéales\" ou attendues pour chaque question, basées sur les connaissances contenues dans les documents sources. C'est la vérité terrain pour la réponse.\n",
        "\n",
        "- reference : Des extraits spécifiques des documents sources qui supportent les ground_truths. Ils représentent le contexte pertinent qui devrait idéalement être récupéré.\n",
        "\n",
        "- answers : Les réponses réellement générées par notre système RAG pour chaque question.\n",
        "\n",
        "- retrieved_contexts : Les morceaux de document (chunks) réellement récupérés par le retriever de notre système RAG pour chaque question.\n",
        "\n",
        "En comparant les answers et retrieved_contexts produits par notre système avec les ground_truths et reference prédéfinis, ragas peut évaluer la fidélité de la réponse, la pertinence du contexte récupéré, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROuOlaTV7tno",
        "outputId": "2b649704-49d3-4ee5-c6db-b459ba7d2681"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-fd45435cbd56>:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore_disk = Chroma(\n",
            "<ipython-input-5-7bb5e97e2de8>:24: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"define generative ai\",\n",
        "    \"what is the date of rise of genai\"\n",
        "]\n",
        "\n",
        "ground_truths = [\n",
        "    [\"Generative AI refers to artificial intelligence models capable of creating new content, such as text, images, music, or code, often based on patterns learned from large datasets.\"],\n",
        "    [\"The significant rise and public awareness of generative AI occurred recently, notably accelerating in late 2022 with the release of models like ChatGPT.\"]\n",
        "]\n",
        "\n",
        "reference = [\n",
        "    \"Generative AI is a category of artificial intelligence algorithms that generate new outputs based on the data they have been trained on. These models can produce diverse forms of content, including text, images, audio, and synthetic data.\",\n",
        "    \"While generative AI concepts have existed for years, the field experienced a significant surge in capabilities and public attention starting in late 2022, largely driven by the release of highly capable large language models such as OpenAI's ChatGPT.\"\n",
        "]\n",
        "\n",
        "answers = []\n",
        "contexts = []\n",
        "retrieved_contexts = []\n",
        "\n",
        "retriever = get_retriver(gemini_embeddings)\n",
        "# Intégration des réponses du RAG aux questions de test\n",
        "for query in questions:\n",
        "  answers.append(get_llm_response(query))\n",
        "  retrieved_contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
        "\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"reference\": reference,\n",
        "    \"ground_truths\": ground_truths,\n",
        "    \"retrieved_contexts\": retrieved_contexts,\n",
        "}\n",
        "# Construction du dataset de test\n",
        "dataset = Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KbWtvtbPG9Q"
      },
      "source": [
        "##Définition de la fonction de test :\n",
        "\n",
        "Le package `RAGAS` permet de calculer différentes métriques pour juger de la qualité de la récupération d'information et de la génération de réponse :\n",
        "\n",
        "- **Faithfulness (Fidélité) :** Cette métrique évalue la cohérence factuelle de la `answer` générée par rapport aux `retrieved_contexts`. Elle quantifie la proportion des affirmations contenues dans la réponse qui sont directement supportées par les informations extraites des documents sources. Un score élevé indique une faible tendance à l'hallucination, c'est-à-dire que le LLM ne génère pas d'informations non étayées par le contexte fourni.\n",
        "\n",
        "- **Answer Relevancy (Pertinence de la Réponse) :** Cette métrique mesure le degré auquel la `answer` produite par le LLM adresse directement la question initiale de l'utilisateur. Elle évalue si la réponse est pertinente par rapport à l'intention de la question, indépendamment de la qualité du contexte récupéré.\n",
        "\n",
        "- **Context Recall :** Cette métrique détermine la complétude des `retrieved_contexts`. Elle évalue dans quelle mesure les informations essentielles contenues dans les `ground_truths` sont présentes parmi les documents récupérés par le retriever. Un score élevé signifie que le retriever a réussi à extraire la majorité des informations pertinentes nécessaires pour formuler une réponse complète et correcte. Cette métrique requiert la disponibilité des `ground_truths` pour l'évaluation.\n",
        "\n",
        "- **Context Precision (Précision du Contexte) :** Cette métrique évalue la pureté des `retrieved_contexts`. Elle mesure la proportion des documents récupérés qui sont effectivement pertinents pour répondre à la question. Un score élevé indique que le retriever minimise l'inclusion de `bruit` ou d'informations non pertinentes dans l'ensemble des documents passés au LLM.\n",
        "\n",
        "- **Context Entity Recall :** Similaire au **Context Recall**, cette métrique se focalise spécifiquement sur la présence des entités clés identifiées dans les `ground_truths`. Elle permet d'évaluer si le retriever parvient à extraire les éléments nominatifs ou conceptuels centraux liés à la réponse attendue.\n",
        "\n",
        "- **Answer Similarity (Similarité de la Réponse) :** Cette métrique compare la `answer` générée à la `ground_truths` en utilisant des techniques de similarité sémantique. Elle évalue si le sens global de la réponse générée est proche de celui de la `ground_truths`, même si la formulation textuelle diffère.\n",
        "\n",
        "- **Answer Correctness (Exactitude de la Réponse) :** Cette métrique constitue une évaluation globale de l'exactitude factuelle de la `answer` générée par rapport aux `ground_truths`. C'est un indicateur clé de la performance globale du système RAG à produire des réponses véridiques. Cette métrique nécessite des ground_truths fiables pour une évaluation significative.\n",
        "\n",
        "Cette liste de métriques n'est pas exhaustive ; vous pouvez consulter la [documentation officielle de Ragas](https://docs.ragas.io/en/stable/) pour la liste complète et choisir celles qui conviennent le mieux à l'évaluation de votre application spécifique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_6QcRdrHtNR"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    context_entity_recall,\n",
        "    answer_similarity,\n",
        "    answer_correctness\n",
        ")\n",
        "from datasets import Dataset\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def test_performance_rag(dataset):\n",
        "\n",
        "  ragas_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "  gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "  result = evaluate(\n",
        "      dataset,\n",
        "      metrics=[\n",
        "          faithfulness,\n",
        "          context_precision,\n",
        "          context_recall,\n",
        "          answer_relevancy,\n",
        "          context_entity_recall,\n",
        "          answer_similarity,\n",
        "          answer_correctness,\n",
        "      ],\n",
        "      llm=ragas_llm,\n",
        "      embeddings=gemini_embeddings\n",
        "  )\n",
        "\n",
        "  return result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KizSUIOZT7KV"
      },
      "source": [
        "Il est important de noter que la version gratuite de l'API Gemini impose une limite au nombre d'appels par minute (généralement entre 15 et 30 selon le modèle). Étant donné que Ragas effectue des appels intensifs au LLM pour calculer ses métriques, il est facile d'atteindre ce seuil. L'API Google gère automatiquement ce problème en mettant les appels en attente jusqu'à ce que de nouvelles requêtes soient disponibles. Par conséquent, l'exercice d'évaluation s'exécutera jusqu'au bout, mais il risque de prendre considérablement plus de temps, la majorité de ce temps étant consacré à l'attente de la disponibilité de nouveaux appels API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "60687be23bf94ff2b75bc707e809257a",
            "7a158276359143cab8357add37006557",
            "02af4e30a3d54ed9909874ea016f15b6",
            "37c1e947a6e648c4aa8780c95149d76e",
            "834de04f675a430a891cd8eb66568a3c",
            "8a6f56e250964e73bfeab3baa25392a8",
            "ba06972e6b7c438a996599519908cea1",
            "365b68e0cde140768fad86a4dedcdfbb",
            "e518d98b6591403a8c3d429b4e867530",
            "565dd08d30ed4d399f614d7e73eec421",
            "b14b14a5a8564325b80cc78cf7e3b555"
          ]
        },
        "id": "MpfjHUxFIzsd",
        "outputId": "71d389fc-55a4-4816-d09e-48b15e4d13ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60687be23bf94ff2b75bc707e809257a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"what is the date of rise of genai\",\n          \"define generative ai\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"I am sorry, but the provided context does not contain the date of the rise of GenAI. Therefore, I cannot answer your question.\",\n          \"Generative AI models create new content like text, images, or code. These models learn patterns from training data. Generative AI transforms prompts into new content.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"While generative AI concepts have existed for years, the field experienced a significant surge in capabilities and public attention starting in late 2022, largely driven by the release of highly capable large language models such as OpenAI's ChatGPT.\",\n          \"Generative AI is a category of artificial intelligence algorithms that generate new outputs based on the data they have been trained on. These models can produce diverse forms of content, including text, images, audio, and synthetic data.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4714045207910317,\n        \"min\": 0.3333333333333333,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3333333333333333,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7071067811629773,\n        \"min\": 0.0,\n        \"max\": 0.9999999999666667,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          0.9999999999666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7071067811865476,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.49593867870107883,\n        \"min\": 0.0,\n        \"max\": 0.7013632055244585,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          0.7013632055244585\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_entity_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28284271190893356,\n        \"min\": 0.0,\n        \"max\": 0.3999999992,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          0.3999999992\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"semantic_similarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11051944677468561,\n        \"min\": 0.7985044581708641,\n        \"max\": 0.9548025587055959,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7985044581708641,\n          0.9548025587055959\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.38118325228694516,\n        \"min\": 0.19962611454271603,\n        \"max\": 0.738700639676399,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.19962611454271603,\n          0.738700639676399\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "result"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3f552320-18fa-40f5-8e42-64d1933a4075\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_entity_recall</th>\n",
              "      <th>semantic_similarity</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>define generative ai</td>\n",
              "      <td>[transform prompts in new content  (text, imag...</td>\n",
              "      <td>Generative AI models create new content like t...</td>\n",
              "      <td>Generative AI is a category of artificial inte...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.701363</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.954803</td>\n",
              "      <td>0.738701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what is the date of rise of genai</td>\n",
              "      <td>[IBI 2025 Conference © All rights reserved  | ...</td>\n",
              "      <td>I am sorry, but the provided context does not ...</td>\n",
              "      <td>While generative AI concepts have existed for ...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.798504</td>\n",
              "      <td>0.199626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f552320-18fa-40f5-8e42-64d1933a4075')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f552320-18fa-40f5-8e42-64d1933a4075 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f552320-18fa-40f5-8e42-64d1933a4075');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-236fbe26-64d9-4bc5-b42b-97d2beb997dc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-236fbe26-64d9-4bc5-b42b-97d2beb997dc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-236fbe26-64d9-4bc5-b42b-97d2beb997dc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                          user_input  \\\n",
              "0               define generative ai   \n",
              "1  what is the date of rise of genai   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [transform prompts in new content  (text, imag...   \n",
              "1  [IBI 2025 Conference © All rights reserved  | ...   \n",
              "\n",
              "                                            response  \\\n",
              "0  Generative AI models create new content like t...   \n",
              "1  I am sorry, but the provided context does not ...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  Generative AI is a category of artificial inte...      1.000000   \n",
              "1  While generative AI concepts have existed for ...      0.333333   \n",
              "\n",
              "   context_precision  context_recall  answer_relevancy  context_entity_recall  \\\n",
              "0                1.0             1.0          0.701363                    0.4   \n",
              "1                0.0             0.0          0.000000                    0.0   \n",
              "\n",
              "   semantic_similarity  answer_correctness  \n",
              "0             0.954803            0.738701  \n",
              "1             0.798504            0.199626  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = test_performance_rag(dataset)\n",
        "#Afficher les résultats\n",
        "result.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02af4e30a3d54ed9909874ea016f15b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365b68e0cde140768fad86a4dedcdfbb",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e518d98b6591403a8c3d429b4e867530",
            "value": 14
          }
        },
        "365b68e0cde140768fad86a4dedcdfbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c1e947a6e648c4aa8780c95149d76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565dd08d30ed4d399f614d7e73eec421",
            "placeholder": "​",
            "style": "IPY_MODEL_b14b14a5a8564325b80cc78cf7e3b555",
            "value": " 14/14 [00:06&lt;00:00,  1.46it/s]"
          }
        },
        "565dd08d30ed4d399f614d7e73eec421": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60687be23bf94ff2b75bc707e809257a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a158276359143cab8357add37006557",
              "IPY_MODEL_02af4e30a3d54ed9909874ea016f15b6",
              "IPY_MODEL_37c1e947a6e648c4aa8780c95149d76e"
            ],
            "layout": "IPY_MODEL_834de04f675a430a891cd8eb66568a3c"
          }
        },
        "7a158276359143cab8357add37006557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a6f56e250964e73bfeab3baa25392a8",
            "placeholder": "​",
            "style": "IPY_MODEL_ba06972e6b7c438a996599519908cea1",
            "value": "Evaluating: 100%"
          }
        },
        "834de04f675a430a891cd8eb66568a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6f56e250964e73bfeab3baa25392a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14b14a5a8564325b80cc78cf7e3b555": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba06972e6b7c438a996599519908cea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e518d98b6591403a8c3d429b4e867530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
