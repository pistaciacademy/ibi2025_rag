{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41BK2KLDDUdu"
      },
      "source": [
        "# Evaluation des résultats avec RAGAS\n",
        "\n",
        "L'objectif de ce notebook est d'évaluer les résultats du système RAG créé précédemment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAmzqU3hECCt"
      },
      "source": [
        "## Installation des packages nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3CmOqTh7zoO"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain-community\n",
        "!pip install --quiet langchain-google-genai\n",
        "!pip install --quiet chromadb\n",
        "!pip install -q pypdf\n",
        "!pip install -q ragas\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H8jIC5tEY_-"
      },
      "source": [
        "## Import des packages nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9e5gMyXEhBy",
        "outputId": "8d9e72f1-5a89-4030-9785-48746ab551c6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KU1qKgFEwu"
      },
      "source": [
        "## Définition du modèle LLM et du modèle embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZDnvJt5xFFtC"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_API_KEY'] = \"XXXXXXXXXXXX\" # Ici remplacez par votre API Key !!!\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\", temperature=0.7, top_p=0.85)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr5lvkPrF7CO"
      },
      "source": [
        "## Définition du système RAG créé précédemment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o_XfcKp179au"
      },
      "outputs": [],
      "source": [
        "def get_retriver(gemini_embeddings):\n",
        "\n",
        "    vectorstore_disk = Chroma(\n",
        "                         persist_directory=\"./chroma_db\",\n",
        "                         embedding_function=gemini_embeddings\n",
        "                       )\n",
        "    return vectorstore_disk.as_retriever(search_kwargs={\"k\": 3}) # k=3 signifie récupérer les 5 chunks les plus pertinents\n",
        "\n",
        "def get_prompt():\n",
        "\n",
        "    llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use five sentences maximum and keep the answer concise.\\n\n",
        "    Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
        "\n",
        "    return PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def get_llm_response(question):\n",
        "    retriever = get_retriver(gemini_embeddings)\n",
        "    llm_prompt = get_prompt()\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | llm_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain.invoke(question)\n",
        "\n",
        "def load_file(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "    return pages\n",
        "\n",
        "def generate_vectorstore(file_path):\n",
        "\n",
        "    pages = load_file(file_path)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    chunked_documents = text_splitter.split_documents(pages)\n",
        "\n",
        "    print(f\"Chargé {len(pages)} pages et créé {len(chunked_documents)} chunks.\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "                        documents=chunked_documents,\n",
        "                        embedding=gemini_embeddings,\n",
        "                        persist_directory=\"./chroma_db\"\n",
        "                    )\n",
        "\n",
        "    print(f\"Base de données vectorielle créée/mise à jour dans ./chroma_db avec {len(chunked_documents)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoSrk66QGOZI"
      },
      "source": [
        "## Chargement des documents sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvdhITsp9JeR",
        "outputId": "3e86c1d6-d479-40cc-82fc-ed8eb3a28b87"
      },
      "outputs": [],
      "source": [
        "generate_vectorstore(r\"ici il faut remplacer par le path de votre fichier .pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_e7WkARJXCD"
      },
      "source": [
        "## Création de dataset de test\n",
        "\n",
        "Pour évaluer la performance du système RAG, nous devons préparer un dataset de test. Ce dataset permettra au package ragas de calculer différentes métriques pour juger de la qualité de la récupération d'information et de la génération de réponse.\n",
        "\n",
        "Ce dataset est structuré autour de plusieurs éléments :\n",
        "\n",
        "- questions : La liste des questions posées au système RAG.\n",
        "\n",
        "- ground_truths : Les réponses \"idéales\" ou attendues pour chaque question, basées sur les connaissances contenues dans les documents sources. C'est la vérité terrain pour la réponse.\n",
        "\n",
        "- reference : Des extraits spécifiques des documents sources qui supportent les ground_truths. Ils représentent le contexte pertinent qui devrait idéalement être récupéré.\n",
        "\n",
        "- answers : Les réponses réellement générées par notre système RAG pour chaque question.\n",
        "\n",
        "- retrieved_contexts : Les morceaux de document (chunks) réellement récupérés par le retriever de notre système RAG pour chaque question.\n",
        "\n",
        "En comparant les answers et retrieved_contexts produits par notre système avec les ground_truths et reference prédéfinis, ragas peut évaluer la fidélité de la réponse, la pertinence du contexte récupéré, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROuOlaTV7tno",
        "outputId": "2b649704-49d3-4ee5-c6db-b459ba7d2681"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"define generative ai\",\n",
        "    \"what is the date of rise of genai\"\n",
        "]\n",
        "\n",
        "ground_truths = [\n",
        "    [\"Generative AI refers to artificial intelligence models capable of creating new content, such as text, images, music, or code, often based on patterns learned from large datasets.\"],\n",
        "    [\"The significant rise and public awareness of generative AI occurred recently, notably accelerating in late 2022 with the release of models like ChatGPT.\"]\n",
        "]\n",
        "\n",
        "reference = [\n",
        "    \"Generative AI is a category of artificial intelligence algorithms that generate new outputs based on the data they have been trained on. These models can produce diverse forms of content, including text, images, audio, and synthetic data.\",\n",
        "    \"While generative AI concepts have existed for years, the field experienced a significant surge in capabilities and public attention starting in late 2022, largely driven by the release of highly capable large language models such as OpenAI's ChatGPT.\"\n",
        "]\n",
        "\n",
        "answers = []\n",
        "contexts = []\n",
        "retrieved_contexts = []\n",
        "\n",
        "retriever = get_retriver(gemini_embeddings)\n",
        "# Intégration des réponses du RAG aux questions de test\n",
        "for query in questions:\n",
        "  answers.append(get_llm_response(query))\n",
        "  retrieved_contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
        "\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"reference\": reference,\n",
        "    \"ground_truths\": ground_truths,\n",
        "    \"retrieved_contexts\": retrieved_contexts,\n",
        "}\n",
        "# Construction du dataset de test\n",
        "dataset = Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KbWtvtbPG9Q"
      },
      "source": [
        "## Définition de la fonction de test :\n",
        "\n",
        "Le package `RAGAS` permet de calculer différentes métriques pour juger de la qualité de la récupération d'information et de la génération de réponse :\n",
        "\n",
        "- **Faithfulness (Fidélité) :** Cette métrique évalue la cohérence factuelle de la `answer` générée par rapport aux `retrieved_contexts`. Elle quantifie la proportion des affirmations contenues dans la réponse qui sont directement supportées par les informations extraites des documents sources. Un score élevé indique une faible tendance à l'hallucination, c'est-à-dire que le LLM ne génère pas d'informations non étayées par le contexte fourni.\n",
        "\n",
        "- **Answer Relevancy (Pertinence de la Réponse) :** Cette métrique mesure le degré auquel la `answer` produite par le LLM adresse directement la question initiale de l'utilisateur. Elle évalue si la réponse est pertinente par rapport à l'intention de la question, indépendamment de la qualité du contexte récupéré.\n",
        "\n",
        "- **Context Recall :** Cette métrique détermine la complétude des `retrieved_contexts`. Elle évalue dans quelle mesure les informations essentielles contenues dans les `ground_truths` sont présentes parmi les documents récupérés par le retriever. Un score élevé signifie que le retriever a réussi à extraire la majorité des informations pertinentes nécessaires pour formuler une réponse complète et correcte. Cette métrique requiert la disponibilité des `ground_truths` pour l'évaluation.\n",
        "\n",
        "- **Context Precision (Précision du Contexte) :** Cette métrique évalue la pureté des `retrieved_contexts`. Elle mesure la proportion des documents récupérés qui sont effectivement pertinents pour répondre à la question. Un score élevé indique que le retriever minimise l'inclusion de `bruit` ou d'informations non pertinentes dans l'ensemble des documents passés au LLM.\n",
        "\n",
        "- **Context Entity Recall :** Similaire au **Context Recall**, cette métrique se focalise spécifiquement sur la présence des entités clés identifiées dans les `ground_truths`. Elle permet d'évaluer si le retriever parvient à extraire les éléments nominatifs ou conceptuels centraux liés à la réponse attendue.\n",
        "\n",
        "- **Answer Similarity (Similarité de la Réponse) :** Cette métrique compare la `answer` générée à la `ground_truths` en utilisant des techniques de similarité sémantique. Elle évalue si le sens global de la réponse générée est proche de celui de la `ground_truths`, même si la formulation textuelle diffère.\n",
        "\n",
        "- **Answer Correctness (Exactitude de la Réponse) :** Cette métrique constitue une évaluation globale de l'exactitude factuelle de la `answer` générée par rapport aux `ground_truths`. C'est un indicateur clé de la performance globale du système RAG à produire des réponses véridiques. Cette métrique nécessite des ground_truths fiables pour une évaluation significative.\n",
        "\n",
        "Cette liste de métriques n'est pas exhaustive ; vous pouvez consulter la [documentation officielle de Ragas](https://docs.ragas.io/en/stable/) pour la liste complète et choisir celles qui conviennent le mieux à l'évaluation de votre application spécifique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T_6QcRdrHtNR"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    context_entity_recall,\n",
        "    answer_similarity,\n",
        "    answer_correctness\n",
        ")\n",
        "from datasets import Dataset\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def test_performance_rag(dataset):\n",
        "\n",
        "  ragas_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")\n",
        "  gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "  result = evaluate(\n",
        "      dataset,\n",
        "      metrics=[\n",
        "          faithfulness,\n",
        "          context_precision,\n",
        "          context_recall,\n",
        "          answer_relevancy,\n",
        "          context_entity_recall,\n",
        "          answer_similarity,\n",
        "          answer_correctness,\n",
        "      ],\n",
        "      llm=ragas_llm,\n",
        "      embeddings=gemini_embeddings\n",
        "  )\n",
        "\n",
        "  return result.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KizSUIOZT7KV"
      },
      "source": [
        "Il est important de noter que la version gratuite de l'API Gemini impose une limite au nombre d'appels par minute (généralement entre 15 et 30 selon le modèle). Étant donné que Ragas effectue des appels intensifs au LLM pour calculer ses métriques, il est facile d'atteindre ce seuil. L'API Google gère automatiquement ce problème en mettant les appels en attente jusqu'à ce que de nouvelles requêtes soient disponibles. Par conséquent, l'exercice d'évaluation s'exécutera jusqu'au bout, mais il risque de prendre considérablement plus de temps, la majorité de ce temps étant consacré à l'attente de la disponibilité de nouveaux appels API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338,
          "referenced_widgets": [
            "60687be23bf94ff2b75bc707e809257a",
            "7a158276359143cab8357add37006557",
            "02af4e30a3d54ed9909874ea016f15b6",
            "37c1e947a6e648c4aa8780c95149d76e",
            "834de04f675a430a891cd8eb66568a3c",
            "8a6f56e250964e73bfeab3baa25392a8",
            "ba06972e6b7c438a996599519908cea1",
            "365b68e0cde140768fad86a4dedcdfbb",
            "e518d98b6591403a8c3d429b4e867530",
            "565dd08d30ed4d399f614d7e73eec421",
            "b14b14a5a8564325b80cc78cf7e3b555"
          ]
        },
        "id": "MpfjHUxFIzsd",
        "outputId": "71d389fc-55a4-4816-d09e-48b15e4d13ab"
      },
      "outputs": [],
      "source": [
        "result = test_performance_rag(dataset)\n",
        "#Afficher les résultats\n",
        "result.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02af4e30a3d54ed9909874ea016f15b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365b68e0cde140768fad86a4dedcdfbb",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e518d98b6591403a8c3d429b4e867530",
            "value": 14
          }
        },
        "365b68e0cde140768fad86a4dedcdfbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c1e947a6e648c4aa8780c95149d76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565dd08d30ed4d399f614d7e73eec421",
            "placeholder": "​",
            "style": "IPY_MODEL_b14b14a5a8564325b80cc78cf7e3b555",
            "value": " 14/14 [00:06&lt;00:00,  1.46it/s]"
          }
        },
        "565dd08d30ed4d399f614d7e73eec421": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60687be23bf94ff2b75bc707e809257a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a158276359143cab8357add37006557",
              "IPY_MODEL_02af4e30a3d54ed9909874ea016f15b6",
              "IPY_MODEL_37c1e947a6e648c4aa8780c95149d76e"
            ],
            "layout": "IPY_MODEL_834de04f675a430a891cd8eb66568a3c"
          }
        },
        "7a158276359143cab8357add37006557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a6f56e250964e73bfeab3baa25392a8",
            "placeholder": "​",
            "style": "IPY_MODEL_ba06972e6b7c438a996599519908cea1",
            "value": "Evaluating: 100%"
          }
        },
        "834de04f675a430a891cd8eb66568a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6f56e250964e73bfeab3baa25392a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14b14a5a8564325b80cc78cf7e3b555": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba06972e6b7c438a996599519908cea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e518d98b6591403a8c3d429b4e867530": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
