{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOpMCQdxqBFz"
      },
      "source": [
        "# Packager la solution dans une interface Streamlit\n",
        "\n",
        "Voil√†. Vous avez cr√©√© avec succ√®s un assistant de recherche utilisant des donn√©es d'articles scientifiques √† l'aide de Gemini, LangChain et Chroma. Il est maintenant temps de packager cette solution dans une interface Streamlit.\n",
        "\n",
        "**Avertissement :** Cette interface Streamlit est pr√©sent√©e et ex√©cut√©e ici au sein d'un notebook Google Colab √† titre exceptionnel pour en simplifier l'acc√®s et l'utilisation, notamment pour les personnes ne disposant pas d'un environnement Python ou d'un IDE configur√© sur leur machine locale. En r√®gle g√©n√©rale, une application Streamlit est con√ßue pour √™tre ex√©cut√©e en local sur votre ordinateur ou d√©ploy√©e sur un serveur web d√©di√© pour un acc√®s public. Vous pouvez ex√©cuter cette m√™me interface en local en installant les d√©pendances n√©cessaires et en utilisant la commande `streamlit run app.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIpOXzulqLMW"
      },
      "source": [
        "## Installation des packages n√©cessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-Y797foep2Vx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'npm' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme exÔøΩcutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q langchain-community\n",
        "!pip install -q langchain-google-genai\n",
        "!pip install -q chromadb\n",
        "!pip install -q pypdf\n",
        "!pip install -q streamlit\n",
        "!npm install -q localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si vous utlisez ce notebook en local, lancer l'interface streamlit avec la commande d√©di√©e et ne pas avec le package `localtunnel`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V2YmKbT6S3X"
      },
      "source": [
        "## Cr√©ation de l'interface avec Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qslkJNBqvCV",
        "outputId": "dc3ca4f7-8d89-43d3-9aaf-da97eca7e9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import time # Optional: To simulate bot thinking time\n",
        "from rag import get_llm_response, generate_vectorstore\n",
        "import os\n",
        "# --- Configuration ---\n",
        "st.set_page_config(page_title=\"My Basic Chatbot\", layout=\"centered\")\n",
        "st.title(\"ü§ñ Simple Chatbot Interface\")\n",
        "uploaded_pdf_file = st.sidebar.file_uploader(\"Upload the pdf file\", type=\"pdf\", label_visibility=\"collapsed\")\n",
        "\n",
        "# if uploaded_file is None and st.session_state.count ==0:\n",
        "#     st.write(\"Please upload the file to see the final results.\\n\")\n",
        "if 'count' not in st.session_state:\n",
        "\tst.session_state.count = 0\n",
        "\n",
        "if uploaded_pdf_file and st.session_state.count < 1:\n",
        "    with st.spinner(\"Processing...\"):\n",
        "        DIR_NAME = \"input_data\"\n",
        "        if not os.path.exists(DIR_NAME):\n",
        "            os.makedirs(DIR_NAME)\n",
        "\n",
        "        pdf_file_path = os.path.join(DIR_NAME, uploaded_pdf_file.name)\n",
        "\n",
        "        with open(pdf_file_path, \"wb\") as f:\n",
        "            f.write(uploaded_pdf_file.getbuffer())\n",
        "\n",
        "        generate_vectorstore(pdf_file_path)\n",
        "# --- Placeholder for Chatbot Logic ---\n",
        "# Replace this with your actual chatbot function/API call\n",
        "def get_bot_response(user_message):\n",
        "    \"\"\"\n",
        "    Simulates a bot response.\n",
        "    Replace this with your actual chatbot logic (e.g., API call, model inference).\n",
        "    \"\"\"\n",
        "    # Simulate thinking time (optional)\n",
        "    # time.sleep(0.5)\n",
        "\n",
        "    # Basic echo response for demonstration\n",
        "    # return f\"You said: '{user_message}'\"\n",
        "\n",
        "    # Slightly more interactive placeholder\n",
        "    if \"hello\" in user_message.lower():\n",
        "        return \"Hello there! How can I help you today?\"\n",
        "    elif \"how are you\" in user_message.lower():\n",
        "        return \"I'm just a bunch of code, but I'm running smoothly! Thanks for asking.\"\n",
        "    elif \"bye\" in user_message.lower():\n",
        "        return \"Goodbye! Have a great day.\"\n",
        "    else:\n",
        "        return get_llm_response(user_message)\n",
        "\n",
        "# --- Session State Initialization ---\n",
        "# Initialize chat history if it doesn't exist\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "    # Optional: Add a starting welcome message from the assistant\n",
        "    st.session_state.messages.append(\n",
        "        {\"role\": \"assistant\", \"content\": \"Hi! Ask me anything.\"}\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Display Chat History ---\n",
        "st.write(\"--- Conversation History ---\")\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"]) # Use markdown for potential formatting\n",
        "st.write(\"---\") # Separator\n",
        "\n",
        "\n",
        "# --- Handle User Input ---\n",
        "# Use st.chat_input which is designed specifically for chat interfaces\n",
        "prompt = st.chat_input(\"What is up?\")\n",
        "\n",
        "if prompt:\n",
        "    # 1. Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # 2. Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # 3. Get bot response (using placeholder function)\n",
        "    with st.spinner(\"Thinking...\"): # Show a thinking indicator\n",
        "        bot_response = get_bot_response(prompt)\n",
        "\n",
        "    # 4. Add bot response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": bot_response})\n",
        "\n",
        "    # 5. Display assistant response in chat message container\n",
        "    # Use st.rerun() for a smoother update after adding messages\n",
        "    st.rerun()\n",
        "\n",
        "\n",
        "# --- Optional: Add a button to clear history ---\n",
        "if st.button(\"Clear Chat History\"):\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Chat cleared. How can I help?\"}]\n",
        "    st.rerun()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR00nHYK6dZJ"
      },
      "source": [
        "## Impl√©mentation de la solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcx_QNq-q2Tt",
        "outputId": "1d32b635-1707-49f9-ab96-7666e1b5e42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing rag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile rag.py\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "import getpass\n",
        "from langchain import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.prompt_template import format_document\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"XXXXXXXXXXXX\" # Ici remplacez par votre API Key !!!\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\", temperature=0.7, top_p=0.85)\n",
        "\n",
        "def get_retriver(gemini_embeddings):\n",
        "\n",
        "    vectorstore_disk = Chroma(\n",
        "                         persist_directory=\"./chroma_db\",\n",
        "                         embedding_function=gemini_embeddings\n",
        "                       )\n",
        "    return vectorstore_disk.as_retriever(search_kwargs={\"k\": 3}) # k=3 signifie r√©cup√©rer les 5 chunks les plus pertinents\n",
        "\n",
        "def get_prompt():\n",
        "\n",
        "    llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use five sentences maximum and keep the answer concise.\\n\n",
        "    Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
        "\n",
        "    return PromptTemplate.from_template(llm_prompt_template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "def get_llm_response(question):\n",
        "    retriever = get_retriver(gemini_embeddings)\n",
        "    llm_prompt = get_prompt()\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | llm_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain.invoke(question)\n",
        "\n",
        "def load_file(file_path):\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    pages = loader.load()\n",
        "    return pages\n",
        "\n",
        "def generate_vectorstore(file_path):\n",
        "\n",
        "    pages = load_file(file_path)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    chunked_documents = text_splitter.split_documents(pages)\n",
        "\n",
        "    print(f\"Charg√© {len(pages)} pages et cr√©√© {len(chunked_documents)} chunks.\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "                        documents=chunked_documents,\n",
        "                        embedding=gemini_embeddings,\n",
        "                        persist_directory=\"./chroma_db\"\n",
        "                    )\n",
        "\n",
        "    print(f\"Base de donn√©es vectorielle cr√©√©e/mise √† jour dans ./chroma_db avec {len(chunked_documents)} chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tUaf2iC6n9c"
      },
      "source": [
        "## Lancement de la solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_iyUr7Sq8nX",
        "outputId": "390936bd-d3bd-40e8-c4f4-fedecb1f00a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "& ÔøΩtait inattendu.\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl https://loca.lt/mytunnelpassword"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF_LABS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
